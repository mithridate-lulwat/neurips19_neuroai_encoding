{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cell, please change the various path to the ones corresponding to your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CHANGE THE PATH TO CORRESPOND TO THE PATH ON YOUR SYSTEM\n",
    "### The path that should be changed are preceded with <--- CHANGE --->\n",
    "\n",
    "def initialize(movie):\n",
    "    # Generate folders for organized storage\n",
    "    folders = [\"fmri_mean_{}\", \"wards_{}\", \"fmri_ready_{}\", \"results\"]\n",
    "    for folder in folders :\n",
    "        folder_name = folder.format(movie)\n",
    "        try :\n",
    "            os.mkdir(folder_name)\n",
    "        except OSError :\n",
    "            print(\"Folder already exists, skipping creation ({})\".format(folder_name))\n",
    "        else :\n",
    "            print(\"Folder created  ({})\".format(folder_name))\n",
    "\n",
    "    result_folders = [\"results/{}_conv{}\".format(movie, id_layer) for id_layer in range(1,8)]\n",
    "    for result_folder in result_folders :\n",
    "        try :\n",
    "            os.mkdir(result_folder)\n",
    "        except OSError :\n",
    "            print(\"Folder already exists, skipping creation ({})\".format(result_folder))\n",
    "        else :\n",
    "            print(\"Folder created ({})\".format(result_folder))\n",
    "    \n",
    "    # Set the differents path to data and folders\n",
    "\n",
    "    # path to dataset of Sherlock or Merlin or TwilightZone\n",
    "    if movie in (\"sherlock\",\"merlin\"):\n",
    "        # fmri data location\n",
    "        \n",
    "        # Location of the dataset\n",
    "        # <--- CHANGE --->\n",
    "        local_movie_path = \"/home/brain/datasets/SherlockMerlin_ds001110/\"\n",
    "        \n",
    "        # mask name\n",
    "        movie_mask =  \"{}Movie_bold_space-T1w_brainmask.nii.gz\".format(movie.capitalize())\n",
    "        \n",
    "        # Generic name of the brain masks (non MNI) from the dataset\n",
    "        # <--- CHANGE --->\n",
    "        generic_mask_name = \"/home/brain/datasets/SherlockMerlin_ds001110/sub-{:02d}/func/sub-{:02d}_task-\" + movie_mask\n",
    "        \n",
    "        # fmri file name\n",
    "        generic_filename = \"sub-{:02d}_task-\" + \"{}Movie_bold_space-T1w_preproc.nii.gz\".format(movie.capitalize())\n",
    "        \n",
    "        # anat file name\n",
    "        \n",
    "        # <--- CHANGE --->\n",
    "        anat_filename = \"/home/brain/victor/datasets/fmri_anat_{}/\".format(movie) + \"sub-{:02d}.nii.gz\"\n",
    "        \n",
    "    elif movie == \"twilight-zone\" :\n",
    "        # fmri data location\n",
    "        # <--- CHANGE --->\n",
    "        local_movie_path = \"/home/brain/victor/datasets/twilight-zone\"\n",
    "        \n",
    "        # there are no pre-existing masks\n",
    "        generic_mask_name = \"\"\n",
    "        \n",
    "        # fmri file name\n",
    "        generic_filename = \"sub-{:02d}_task-watchmovie_bold.nii.gz\"\n",
    "    else :\n",
    "        raise ValueError(\"The movie name has to be 'twilight-zone', 'merlin' or 'sherlock'\")\n",
    "\n",
    "        \n",
    "    # locate the folder containing feature vectors extracted from soundnet for the corresponding movie (merlin_pytorch or sherlock_pytorch\n",
    "    # <--- CHANGE --->\n",
    "    feature_folder = \"soundnet_features/{}_pytorch/\".format(movie)\n",
    "\n",
    "    # folder for storing the resulting r2 brain maps\n",
    "    folder_name = \"results/{}_\".format(movie)\n",
    "    result_folder = folder_name + \"conv{}/\"\n",
    "    \n",
    "    # Configure the subjects corresponding to the movie : \n",
    "    # Sherlock : [1,18]\\{5} \n",
    "    # Merlin : [19,37]\\{25}\n",
    "    # Twilight-zone : [1,25]\n",
    "    id_subjects = {\n",
    "    \"sherlock\": (1,18,5),\n",
    "    \"merlin\": (19,37,25),\n",
    "    \"twilight-zone\": (1,25,0)\n",
    "    }\n",
    "\n",
    "    sub_values = id_subjects[movie]\n",
    "    \n",
    "    return local_movie_path, generic_mask_name, generic_filename, anat_filename, feature_folder, result_folder, sub_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# nilearn imports\n",
    "from nilearn.plotting import plot_roi, plot_stat_map, plot_anat, view_img, cm\n",
    "from nilearn.image import mean_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove useless warnings from scikit learn\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# utility imports\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os.path\n",
    "from joblib import dump, load\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, labels, fv_vector):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.vectors = fv_vector\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.vectors)\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # Load data and get label\n",
    "        X = self.vectors[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions import Parcellations\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_background_mask\n",
    "import os\n",
    "\n",
    "def parcellate(id_subject, n_frames, compute_mean = True, compute_ward = True, compute_ready = True):\n",
    "    # Compute the ward parcellation and other fmri data of a given subject and save it\n",
    "    \n",
    "    folder_name = \"sub-{:02d}/func\".format(id_subject)\n",
    "    subject_filename = generic_filename.format(id_subject)\n",
    "    fmri_file = os.path.join(local_movie_path,folder_name, subject_filename)\n",
    "\n",
    "    # Compute the mean of fmri accross time\n",
    "    if  compute_mean or not os.path.isfile(\"fmri_mean_{}/sub-{:02d}.nii.gz\".format(movie, id_subject)):\n",
    "        fmri_mean = mean_img(fmri_file)\n",
    "        fmri_mean.to_filename(\"fmri_mean_{}/sub-{:02d}.nii.gz\".format(movie, id_subject))\n",
    "        print(\"Saved mean fmri for subject {}\".format(id_subject))\n",
    "    else :\n",
    "        print(\"Mean fmri already exists for subject {}\".format(id_subject))\n",
    "        fmri_mean = \"fmri_mean_{}/sub-{:02d}.nii.gz\".format(movie, id_subject)\n",
    " \n",
    "    # Compute mask if the movie is twilight-zone \n",
    "    if movie == \"twilight-zone\":\n",
    "        print(\"Computing background mask\")\n",
    "        mask_img = compute_background_mask(fmri_file)\n",
    "    else :\n",
    "        print(\"Loading pre-generated mask\")\n",
    "        mask_img = generic_mask_name.format(id_subject,id_subject)\n",
    "    \n",
    "    # Compute ward parcellation\n",
    "    if compute_ward or not os.path.isfile(\"wards_{}/sub-{:02d}.nii.gz\".format(movie,id_subject)):\n",
    "        masker = NiftiMasker(mask_img=mask_img, detrend=True,standardize=True)\n",
    "        masker.fit()\n",
    "        ward = Parcellations(method='ward',mask=masker,standardize=True,smoothing_fwhm=None,n_parcels=500)\n",
    "        ward.fit(fmri_file)\n",
    "        dump(ward, \"wards_{}/sub-{:02d}.nii.gz\".format(movie,id_subject))\n",
    "        print(\"Saved ward mask for subject {}\".format(id_subject))\n",
    "    else :\n",
    "        ward = load(\"wards_{}/sub-{:02d}.nii.gz\".format(movie,id_subject))\n",
    "        print(\"Ward mask exists for subject {}\".format(id_subject))\n",
    "\n",
    "     # Compute fmri_ready       \n",
    "    if compute_ready or not os.path.isfile(\"fmri_ready_{}/sub-{:02d}.npy\".format(movie, id_subject)):\n",
    "        print(\"fmri_file : \", fmri_file)\n",
    "        fmri_data = ward.transform(fmri_file)\n",
    "        # Truncate the data because of an offset in the fmri (see dataset description)\n",
    "        if movie in (\"merlin, sherlock\"):\n",
    "            # 25 seconds of offset, and 1.5s per frame\n",
    "            fmri_ready = fmri_data[17:-(fmri_data.shape[0]-17-n_frames)]\n",
    "        else :\n",
    "            # 15 TR of offset, and 1.5s per frame\n",
    "            fmri_ready = fmri_data[15:-(fmri_data.shape[0]-15-n_frames)]\n",
    "        np.save(\"fmri_ready_{}/sub-{:02d}\".format(movie, id_subject), fmri_ready)    \n",
    "        print(\"Saved fmri_ready for subject {}\".format(id_subject))\n",
    "    else :\n",
    "        print(\"fmri_ready exists for subject {} and movie {}\".format(id_subject, movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_vector(id_layer) :\n",
    "    filename = \"conv{}.npz\".format(id_layer)\n",
    "    file_fv = os.path.join(feature_folder, filename)\n",
    "    fv = np.load(file_fv)['fv'][1:]\n",
    "    # Check the size\n",
    "    n_frames = fv.shape[0]\n",
    "    fv_normalized = StandardScaler().fit_transform(fv)\n",
    "    print(\"layer {}, {} frames, FV dimension is {}\".format(id_layer,n_frames, fv.shape[1]))\n",
    "    return fv_normalized, n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmri_data(id_subject):\n",
    "    fmri_ready_path = \"fmri_ready_{}/sub-{:02d}.npy\".format(movie, id_subject)\n",
    "    fmri_ready = np.load(fmri_ready_path)[1:]\n",
    "    print(\"Loaded {}, with length : {}\".format(fmri_ready_path, len(fmri_ready)))\n",
    "    \n",
    "    fmri_mean_path = \"fmri_mean_{}/sub-{:02d}.nii.gz\".format(movie, id_subject)\n",
    "    print(\"Loaded {}\".format(fmri_mean_path))\n",
    "    \n",
    "    ward_path = \"wards_{}/sub-{:02d}.nii.gz\".format(movie, id_subject)\n",
    "    ward = load(ward_path)\n",
    "    print(\"Loaded {}\".format(ward_path))\n",
    "    \n",
    "    return fmri_ready, fmri_mean_path, ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(id_layer,id_subject): \n",
    "    X, _ = load_feature_vector(id_layer)\n",
    "    X_tensor = torch.tensor(X, dtype = torch.float, requires_grad = True)\n",
    "    y, fmri_mean, ward = load_fmri_data(id_subject)\n",
    "    y_tensor = torch.tensor(y, dtype = torch.float, requires_grad = True)\n",
    "    return X, y, fmri_mean, ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(1024, 1000)  \n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.cpu()\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(id_subject, tol = 1e-4, n_iter_no_change=10) :\n",
    "    \"\"\"tol : float, optional, default 1e-4\n",
    "        Tolerance for the optimization. When the loss or score is not improving\n",
    "        by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
    "        convergence is considered to be reached and training stops.\n",
    "        \n",
    "        n_iter_no_change : int, optional, default 10\n",
    "        Maximum number of epochs to not meet ``tol`` improvement.\n",
    "        \"\"\"\n",
    "    X,y,fmri_mean, ward = load_data(id_layer = 7, id_subject = id_subject)\n",
    "    result_folder_layer = result_folder.format(7)\n",
    "    fold_number = 1\n",
    "    views = []\n",
    "\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        ### Create a NEW neural network\n",
    "        net = Net()\n",
    "        net.to(device)\n",
    "        # Generate data\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "        train_length = X_train.shape[0]\n",
    "        validation_length = X_val.shape[0]\n",
    "        testing_length = X_test.shape[0]\n",
    "        training_set = Dataset(y_train, X_train)\n",
    "        validation_set = Dataset(y_val, X_val)\n",
    "        testing_set = Dataset(y_test, X_test)\n",
    "        training_generator = data.DataLoader(training_set, **params)\n",
    "        validation_generator = data.DataLoader(validation_set, **params)\n",
    "        testing_generator = data.DataLoader(testing_set, **params)\n",
    "        # Create optimizer\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay = 0.0001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Prepare logging\n",
    "        train_losses = []\n",
    "        validation_losses = []\n",
    "        best_validation_loss = 1e10\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for epoch in range(1,max_epochs+1) :\n",
    "            # Training\n",
    "            minibatch_number = 1\n",
    "            loss_sum = 0\n",
    "            for local_batch, local_labels in training_generator:\n",
    "                optimizer.zero_grad()   # zero the gradient buffers\n",
    "                output = net(local_batch)\n",
    "                loss = criterion(output, local_labels.float())\n",
    "                loss.backward()\n",
    "                optimizer.step() # Does the update\n",
    "                loss_sum += loss.item()\n",
    "                minibatch_number += 1\n",
    "            train_loss = loss_sum / train_length\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            minibatch_number = 1\n",
    "            loss_sum = 0\n",
    "            with torch.set_grad_enabled(False):\n",
    "                for local_batch, local_labels in validation_generator:\n",
    "                    output = net(local_batch)\n",
    "                    loss = criterion(output, local_labels.float())\n",
    "                    loss_sum += loss.item()\n",
    "                    minibatch_number += 1\n",
    "            validation_loss = loss_sum / validation_length\n",
    "            validation_losses.append(validation_loss)\n",
    "            \n",
    "            # Check for early-stopping\n",
    "            if validation_loss > (best_validation_loss - tol):\n",
    "                no_improvement_count += 1\n",
    "            else :\n",
    "                no_improvement_count = 0\n",
    "            if validation_loss < best_validation_loss :\n",
    "                best_validation_loss = validation_loss\n",
    "            if epoch % 5 == 0 :\n",
    "                print(\"Fold {} epoch {}\".format(fold_number, epoch))\n",
    "            if no_improvement_count > n_iter_no_change :\n",
    "                # The loss hasn't improved by 'tol' for more than 'n_iter_no_change' epochs\n",
    "                print(\"Early stopping at epoch {}\".format(epoch))\n",
    "                break\n",
    "\n",
    "        # Testing\n",
    "        minibatch_number = 1\n",
    "        loss_sum = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch, local_label in testing_generator:\n",
    "                output = net(local_batch)\n",
    "                loss = criterion(output, local_label.float())\n",
    "                #predictions.append(output.detach().numpy())\n",
    "                #local_labels.append(local_label)\n",
    "                loss_sum += loss.item()\n",
    "                minibatch_number += 1\n",
    "            test_loss = loss_sum / testing_length\n",
    "        print(\"Fold {} testing loss : {}\".format(fold_number, test_loss))\n",
    "        \n",
    "        # Represent R2 scores\n",
    "        predictions_tensor = net(torch.tensor(X_test, dtype=torch.float))\n",
    "        predictions = predictions_tensor.detach().numpy()\n",
    "        scores_mlp = r2_score(y_test, predictions, multioutput='raw_values')\n",
    "        scores_mlp[scores_mlp < 0] = 0\n",
    "        scores_img = ward.inverse_transform(scores_mlp.reshape((1,-1)))\n",
    "        scores_img.to_filename(\"./{}/fold{}_sub{:02d}.nii.gz\".format(result_folder_layer ,fold_number, id_subject))\n",
    "        fmri_anat = anat_filename.format(id_subject)\n",
    "        plot_stat_map(scores_img, bg_img = fmri_anat, dim = -0.5)\n",
    "        plt.savefig(\"./{}/fold{}_sub{:02d}.png\".format(result_folder_layer ,fold_number, id_subject))\n",
    "        plt.show()\n",
    "                               \n",
    "        # Plot the loss graph\n",
    "        x_axis = [i  for i in range(len(train_losses))]\n",
    "        plt.plot(x_axis, train_losses,)\n",
    "        plt.plot(x_axis, validation_losses,)\n",
    "        plt.plot(x_axis, [test_loss for i in x_axis])\n",
    "        plt.legend([\"train loss\" ,\"validation loss\", \"test loss\"])\n",
    "        plt.show()\n",
    "\n",
    "        fold_number += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHERLOCK \n",
    "\n",
    "# Movie has to be 'sherlock', 'merlin' or 'twilight-zone'\n",
    "movie = 'sherlock'\n",
    "device = torch.device(\"cuda:0\")\n",
    "local_movie_path, generic_mask_name, generic_filename, anat_filename, feature_folder, result_folder, sub_values = initialize(movie)\n",
    "min_sub, max_sub, null_sub = sub_values\n",
    "\n",
    "# Compute parcellation if needed\n",
    "_, n_frames = load_feature_vector(7)\n",
    "for id_subject in range(min_sub, max_sub):\n",
    "    if id_subject != null_sub :\n",
    "        parcellate(id_subject, n_frames, False, False, False)\n",
    "\n",
    "# Train the network\n",
    "import torch.optim as optim\n",
    "n_folds = 4\n",
    "cv = KFold(n_splits = n_folds)\n",
    "\n",
    "\n",
    "# Add labels\n",
    "max_epochs = 1000\n",
    "params = { \n",
    "    'batch_size': 32,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 6\n",
    "}\n",
    "for id_subject in range(min_sub, max_sub):\n",
    "    if id_subject != null_sub :\n",
    "        train(id_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERLIN \n",
    "\n",
    "# Movie has to be 'sherlock', 'merlin' or 'twilight-zone'\n",
    "movie = 'merlin'\n",
    "device = torch.device(\"cuda:0\")\n",
    "local_movie_path, generic_mask_name, generic_filename, anat_filename, feature_folder, result_folder, sub_values = initialize(movie)\n",
    "min_sub, max_sub, null_sub = sub_values\n",
    "\n",
    "# Compute parcellation if needed\n",
    "_, n_frames = load_feature_vector(7)\n",
    "for id_subject in range(min_sub, max_sub):\n",
    "    if id_subject != null_sub :\n",
    "        parcellate(id_subject, n_frames, False, False, False)\n",
    "\n",
    "# Train the network\n",
    "import torch.optim as optim\n",
    "n_folds = 4\n",
    "cv = KFold(n_splits = n_folds)\n",
    "\n",
    "\n",
    "# Add labels\n",
    "max_epochs = 1000\n",
    "params = { \n",
    "    'batch_size': 32,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 6\n",
    "}\n",
    "for id_subject in range(min_sub, max_sub):\n",
    "    if id_subject != null_sub :\n",
    "        train(id_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
